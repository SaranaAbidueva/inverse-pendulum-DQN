{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import sys # Для sys.exit()\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.21)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Эпизод: 0/250, продолжительность игры в кадрах: 14\n",
      "Эпизод: 1/250, продолжительность игры в кадрах: 26\n",
      "Эпизод: 2/250, продолжительность игры в кадрах: 15\n",
      "Эпизод: 3/250, продолжительность игры в кадрах: 17\n",
      "Эпизод: 4/250, продолжительность игры в кадрах: 11\n",
      "Эпизод: 5/250, продолжительность игры в кадрах: 14\n",
      "Эпизод: 6/250, продолжительность игры в кадрах: 23\n",
      "Эпизод: 7/250, продолжительность игры в кадрах: 24\n",
      "Эпизод: 8/250, продолжительность игры в кадрах: 18\n",
      "Эпизод: 9/250, продолжительность игры в кадрах: 26\n",
      "Эпизод: 10/250, продолжительность игры в кадрах: 10\n",
      "Эпизод: 11/250, продолжительность игры в кадрах: 15\n",
      "Эпизод: 12/250, продолжительность игры в кадрах: 15\n",
      "Эпизод: 13/250, продолжительность игры в кадрах: 10\n",
      "Эпизод: 14/250, продолжительность игры в кадрах: 14\n",
      "Эпизод: 15/250, продолжительность игры в кадрах: 15\n",
      "Эпизод: 16/250, продолжительность игры в кадрах: 29\n",
      "Эпизод: 17/250, продолжительность игры в кадрах: 15\n",
      "Эпизод: 18/250, продолжительность игры в кадрах: 18\n",
      "Эпизод: 19/250, продолжительность игры в кадрах: 18\n",
      "Эпизод: 20/250, продолжительность игры в кадрах: 14\n",
      "Эпизод: 21/250, продолжительность игры в кадрах: 33\n",
      "Эпизод: 22/250, продолжительность игры в кадрах: 17\n",
      "Эпизод: 23/250, продолжительность игры в кадрах: 22\n",
      "Эпизод: 24/250, продолжительность игры в кадрах: 13\n",
      "Эпизод: 25/250, продолжительность игры в кадрах: 11\n",
      "Эпизод: 26/250, продолжительность игры в кадрах: 11\n",
      "Эпизод: 27/250, продолжительность игры в кадрах: 30\n",
      "Эпизод: 28/250, продолжительность игры в кадрах: 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5510fa95c715>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Графическое отображение симуляции\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mframes\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Определяем очередное действие\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rgb_array\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mglClearColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Агент с глубоким Q-обучением\n",
    "class DQNAgent():\n",
    "    #\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.state_size = observation_space\n",
    "        self.action_size = action_space\n",
    "        self.memory = deque(maxlen = 20000) # Тип collections.deque\n",
    "        self.alpha = 1.0 # Скорость обучения агента\n",
    "        self.gamma = 0.95 # Коэффициент уменьшения вознаграждения агента\n",
    "        # Уровень обучения повышается с коэффициентом exploration_decay\n",
    "        # Влияет на выбор действия action (0 или 1)\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_min = 0.01\n",
    "        self.exploration_decay = 0.995\n",
    "        self.learning_rate = 0.001 # Скорость обучения сети\n",
    "        self.model = self.build_model()\n",
    "        print(self.model.summary())\n",
    "    #\n",
    "    # Создает модель сети\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = self.state_size, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(2, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    #\n",
    "    # Запоминаем историю\n",
    "    def remember(self, state, action, reward, state_next, done):\n",
    "        self.memory.append((state, action, reward, state_next, done))\n",
    "    #\n",
    "    # Определяет и возвращает действие\n",
    "    def findAction(self, state):\n",
    "        # Случайный выбор действия - 0 или 1\n",
    "        if np.random.rand() <= self.exploration_rate: return random.randrange(self.action_size) # или random.randint(0, 1)\n",
    "        # Выбор действия по состоянию объекта\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0]) # Возвращает действие - индекс наибольшей оценки полезности действия\n",
    "    #\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size: return\n",
    "        # Обучение агента\n",
    "        # Случайная выборка batch_size элементов для обучения агента\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, state_next, done in minibatch:\n",
    "            # Пример (done = False):\n",
    "            # state: [[-0.00626842 0.41118423 -0.07340495 -0.77232979]]\n",
    "            # q_values (до корректировки): [[0.052909 0.05275263]] - numpy.ndarray\n",
    "            # state_next: [[ 0.00195526 0.21714493 -0.08885155 -0.50361631]]\n",
    "            # q_values_next: [[0.03970249 0.02732118]]\n",
    "            # Qsa = 1.0377173654735088\n",
    "            # reward = 1.0\n",
    "            # action = 0\n",
    "            # q_values (после корректировки): [[1.0377173 0.05275263]]\n",
    "            # q_values (после обучения НС): [[0.07063997 0.04742151]]\n",
    "            q_values = self.model.predict(state)\n",
    "            if done:\n",
    "                Qsa = reward\n",
    "            else:\n",
    "                q_values_next = self.model.predict(state_next)[0]\n",
    "                # Текущая оценка полезности действия action\n",
    "                Qsa = q_values[0][action]\n",
    "                # Уточненная оценка полезности действия action\n",
    "                Qsa = Qsa + self.alpha * (reward + self.gamma * np.amax(q_values_next) - Qsa)  # уравнение Беллмана\n",
    "            # Формируем цель обучения сети\n",
    "            q_values[0][action] = Qsa\n",
    "            # Обучение сети\n",
    "            self.model.fit(state, q_values, epochs = 1, verbose = 0)\n",
    "        if self.exploration_rate > self.exploration_min: self.exploration_rate *= self.exploration_decay\n",
    "#\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1') # Создаем среду. Тип: # <TimeLimit<CartPoleEnv<CartPole-v1>>>\n",
    "    observation_space = env.observation_space.shape[0] # 4\n",
    "    action_space = env.action_space.n # 2\n",
    "    # DQN - глубокая Q-нейронная сеть\n",
    "    dqn_agent = DQNAgent(observation_space, action_space) # Создаем агента\n",
    "    episodes = 251 # Число игровых эпизодов + 1\n",
    "    # scores - хранит длительность игры в последних 100 эпизодах\n",
    "    # После достижения maxlen новые значения, добавляемые в scores, будут вытеснять прежние\n",
    "    scores = deque(maxlen = 100) # Тип collections.deque.\n",
    "    fail = True\n",
    "    seed = 2\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    for e in range(episodes):\n",
    "        # Получаем начальное состояние объекта перед началом каждой игры (каждого эпизода)\n",
    "        state = env.reset() # Как вариант: state = [0.0364131 -0.02130403 -0.03887796 -0.01044108]\n",
    "        # state[0] - позиция тележки\n",
    "        # state[1] - скорость тележки\n",
    "        # state[2] - угол отклонения шеста от вертикали в радианах\n",
    "        # state[3] - скорость изменения угла наклона шеста\n",
    "        state = np.reshape(state, (1, observation_space))\n",
    "        # Начинаем игру\n",
    "        # frame - текущий кадр (момент) игры\n",
    "        # Цель - как можно дольше не допустить падения шеста\n",
    "        frames = 0\n",
    "        while True:\n",
    "            env.render() # Графическое отображение симуляции\n",
    "            frames += 1\n",
    "            action = dqn_agent.findAction(state) # Определяем очередное действие\n",
    "            # Получаем от среды, в которой выполнено действие action, состояние объекта, награду и значение флага завершения игры\n",
    "            # В каждый момент игры, пока не наступило одно из условий ее прекращения, награда равна 1\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            state_next = np.reshape(state_next, (1, observation_space))\n",
    "            reward = reward if not done else -reward\n",
    "            # Запоминаем предыдущее состояние объекта, действие, награду за это действие, текущее состояние и значение done\n",
    "            dqn_agent.remember(state, action, reward, state_next, done)\n",
    "            state = state_next # Обновляем текущее состояние\n",
    "            # done становится равным True, когда завершается игра, например, отклонение угла превысило допустимое значение\n",
    "            if done:\n",
    "                # Печатаем продолжительность игры и покидаем внутренний цикл while\n",
    "                print(\"Эпизод: {}/{}, продолжительность игры в кадрах: {}\".format(e, episodes - 1, frames))\n",
    "                break\n",
    "        scores.append(frames)\n",
    "        if e > 100:\n",
    "            score_mean = np.mean(scores)\n",
    "            if score_mean > 195:\n",
    "                print('Цель достигнута. Средняя продолжительность игры: ', score_mean)\n",
    "                fail = False\n",
    "                break\n",
    "        # Продолжаем обучать агента\n",
    "        dqn_agent.replay(24)\n",
    "    if fail: print('Задача не решена ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
